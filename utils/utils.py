import os, json, time
from langchain_core.runnables import Runnable
from langchain_core.runnables.base import RunnableSequence
from httpx import HTTPStatusError
from langchain_core.exceptions import OutputParserException
from langchain_core.callbacks import CallbackManagerForLLMRun
from typing import Any, Dict, List, Optional
from icecream import ic
from langgraph.graph import StateGraph
from langgraph.graph.graph import CompiledGraph
from IPython.display import Image
from streamlit import stop
from langchain_core.pydantic_v1 import SecretStr
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.language_models.llms import BaseLLM


def set_env():
    def os_param_not_set(param: str) -> bool:
        return os.environ.get(param) is None or os.environ.get(param) == ""

    def set_proxy():
        # a function to test the connectivity to the proxy
        def get_proxy():
            ora_proxy = "http://www-proxy-ash7.us.oracle.com:80"
            import requests

            try:
                requests.get(ora_proxy, timeout=1)
                return ora_proxy
            except Exception as e:
                return ""

        proxy = get_proxy()
        os.environ["http_proxy"] = proxy
        os.environ["https_proxy"] = proxy
        os.environ["no_proxy"] = proxy

    from dotenv import load_dotenv

    load_dotenv()
    if os_param_not_set("LANGCHAIN_API_KEY"):
        os.environ["LANGCHAIN_TRACING_V2"] = ""
        os.environ["LANGCHAIN_ENDPOINT"] = ""
        os.environ["LANGCHAIN_PROJECT"] = ""

    os.environ["LANGCHAIN_PROJECT"] = "rephraser data generator"
    set_proxy()


def get_llm(
    vendor: str = "openai",
    model="gpt-4o-mini",
    temperature: float = 0.7,
    max_tokens: int|None = None,
    top_p: float = 1.0,
    top_k: float|None = None,
    frequency_penalty: float|None = None,
) -> BaseChatModel|BaseLLM:
    match vendor:
        case "groq":
            if model is None or model.strip() == "":
                model = "llama-3.1-70b-versatile"
            from langchain_groq import ChatGroq

            llm = ChatGroq(
                name="ChatGroq LLM",
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                stop_sequences=None,
                model_kwargs={"top_p": top_p, "frequency_penalty": frequency_penalty},
            )
        case "openai":
            if model is None or model.strip() == "":
                model = "gpt-4o-mini"
            from langchain_openai import ChatOpenAI

            llm = ChatOpenAI(
                name="ChatOpenAI LLM",
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                model_kwargs={},
            )
        case "deepinfra":
            # from langchain_community.llms.deepinfra import DeepInfra
            from langchain_openai import ChatOpenAI

            llm = ChatOpenAI(
                name="DeepInfra LLM",
                base_url="https://api.deepinfra.com/v1/openai",
                model=model,
                api_key=SecretStr(os.environ["DEEPINFRA_API_TOKEN"]),
                temperature=temperature,
                max_tokens=max_tokens,
                model_kwargs={
                    "top_p": top_p,
                    "top_k": top_k,
                    "frequency_penalty": frequency_penalty,
                },
            )
        case "oci":
            from langchain_community.llms.oci_generative_ai import OCIGenAI
            import oci, time

            class OneSearchOCIGenAI(OCIGenAI):
                model_kwargs: Dict[str, Any] = {}
                def __init__(self, **kwargs):
                    super().__init__(**kwargs)
                    self.provider = "cohere"

                def _call(
                    self,
                    prompt: str,
                    stop: Optional[List[str]] = None,
                    run_manager: Optional[CallbackManagerForLLMRun] = None,
                    **kwargs: Any,
                ) -> str:
                    """Call out to OCIGenAI generate endpoint.

                    Args:
                        prompt: The prompt to pass into the model.
                        stop: Optional list of stop words to use when generating.

                    Returns:
                        The string generated by the model.

                    Example:
                        .. code-block:: python

                        response = llm.invoke("Tell me a joke.")
                    """
                    # import ipdb; ipdb.set_trace()

                    compartment_id = self.compartment_id
                    model_id = self.model_id
                    generative_ai_inference_client = self.client

                    chat_detail = oci.generative_ai_inference.models.ChatDetails()

                    chat_request = (
                        oci.generative_ai_inference.models.CohereChatRequest()
                    )

                    # import pdb; pdb.set_trace()
                    chat_request.message = prompt
                    chat_request.max_tokens = self.model_kwargs.get("max_tokens", 128) or 128
                    chat_request.is_stream = True
                    chat_request.temperature = self.model_kwargs.get("temperature", 0.7) or 0.7
                    chat_request.top_p = self.model_kwargs.get("top_p")
                    chat_request.top_k = self.model_kwargs.get(
                        "top_k"
                    )  # Only support topK within [0, 500]
                    chat_request.frequency_penalty = self.model_kwargs.get(
                        "frequency_penalty"
                    )

                    chat_detail.serving_mode = (
                        oci.generative_ai_inference.models.OnDemandServingMode(
                            model_id=model_id
                        )
                    )
                    chat_detail.chat_request = chat_request
                    chat_detail.compartment_id = compartment_id
                    chat_response = generative_ai_inference_client.chat(chat_detail)

                    for event in chat_response.data.events():
                        res = json.loads(event.data)
                        if "finishReason" in res.keys():
                            #     print(f"\nFinish reason: {res['finishReason']}")
                            #     print(vars(chat_response))
                            response = res["text"]
                            return response
                    return ""

            llm = OneSearchOCIGenAI(
                model_id=os.environ.get("OCI_MODEL_ID"),
                service_endpoint=os.environ.get("OCI_SERVICE_ENDPOINT"),
                compartment_id=os.environ.get("OCI_COMPARTMENT_ID"),
                auth_type="API_KEY",
                auth_profile=os.environ.get("OCI_AUTH_PROFILE"),
                model_kwargs={
                    "temperature": temperature,
                    "top_p": top_p,
                    "max_tokens": max_tokens,
                    "top_k": top_k,
                    "frequency_penalty": frequency_penalty,
                },
            )
        case "mistralai":
            from langchain_mistralai.chat_models import ChatMistralAI

            llm = ChatMistralAI(
                name="ChatMistralAI LLM",
                api_key=SecretStr(os.environ["MISTRALAI_API_KEY"]),
                model_name=model,
                temperature=temperature
                / 2.0,  # MistralAI uses a different temperature scale, from 0 to 1
                max_tokens=max_tokens,
                top_p=top_p,
            )
        case _:
            raise ValueError(f"Unsupported vendor: {vendor}")
    return llm


def invoke_chain(
    chain: RunnableSequence,
    input: Dict[str, str],
    stop_sequences: Optional[List[str]] = None,
    MAX_TRY: int = 3,
    retry_after: int = 60,
) -> Dict[str, str]:
    i = 0
    response = None
    while i < MAX_TRY:
        i += 1
        try:
            response = chain.invoke(input=input)
            break
        except HTTPStatusError as ex:
            if ex.response.status_code == 429:
                # Too Many Requests error handling
                print(f"Rate limit exceeded. Retrying after {retry_after} seconds.")
                time.sleep(retry_after)
            else:
                # Handle other HTTPStatusError exceptions
                print(f"HTTP error occurred: {ex}")
            continue
        except OutputParserException as ex:
            ic(ex)
            continue
        except Exception as ex:
            ic(ex)
            continue
    return response or {}


def read_prompt_from_jsonl(jsonl_file: str, prompt_name: str):
    prompts = List[Dict[str, str]]
    with open(jsonl_file, "r") as f:
        jsonl = f.readlines()
    prompts = [json.loads(line) for line in jsonl]
    for prompt in prompts:
        if prompt.get("name", "") == prompt_name:
            prompt_content = prompt.get("content", "")
            # Check if prompt_content is an empty string
            if prompt_content == "":
                raise RuntimeError(f"Prompt content is empty for prompt: {prompt_name}")
            return prompt_content
    raise RuntimeError(f"Prompt not found: {prompt_name}")


def save_graph_image(graph: CompiledGraph, dest_png_path: str):
    img_dir = os.path.abspath(os.path.expanduser(os.path.dirname(dest_png_path)))
    img_file = os.path.basename(dest_png_path)
    dest_png_path = os.path.join(img_dir, img_file)
    try:
        if not os.path.isdir(img_dir):
            os.makedirs(img_dir)
    except OSError as e:
        # output error message
        print("Error: {e}")
        raise ValueError(
            f"Could not create directory for {os.path.dirname(dest_png_path)}!"
        )
    graph.get_graph().draw_png(dest_png_path)

# p = read_prompt_from_jsonl(jsonl_file="data/jailbreak_judge.jsonl", prompt_name="openai_concise_jailbreak_judge")
# print(p)

# oci_model = get_llm(vendor="oci")
# response = oci_model.invoke("Tell me a joke.")
# pass

